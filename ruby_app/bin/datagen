#!/usr/bin/env ruby

require 'rubygems'
require 'parallel'
require 'bundler/setup'
require 'csv'
Bundler.require(:default, :development)

require_relative '../lib/datagen'

# A CLI tool for generating sample datasets
class DataGenCLI < Thor

  NUM_OF_SINES = 10

  desc 'create KEY', 'Generate a test dataset for a given key'
  option :count, required: true, type: :numeric
  option :start_time
  option :end_time
  option :type, enum: %w(sin square line power)
  option :force, alias: 'f', type: :boolean
  option :append, alias: 'a', type: :boolean

  def create(key)
    count = options[:count]
    count = count > 1 ? count - 1 : count # "fix" for off by one error

    operation_start_time = Time.now

    # Start generated dataset right now
    data_start_time = Chronic.parse(options[:start_time]) || Time.now

    # add a millisecond per data point
    data_end_time = Chronic.parse(options[:end_time]) || data_start_time + (0.001 * count)

    graph_type = options[:type]
    client = get_tempodb_client

    # See if this key already exists in TempoDB
    series = check_existing_set(client, key, options)

    initial_count = get_series_count(client, series.key)
    puts "Total data points in key: #{initial_count}"

    say "Creating #{count + 1} data points starting at #{data_start_time}..."

    data = generate_data(data_start_time, data_end_time, count, graph_type: graph_type)

    batch_size = 2000
    processes = 1
    threads = 4
    upload_time_start, upload_time_end = write_in_parallel(data, key, batch_size: batch_size, processes: processes, threads: threads)

    operation_end_time = Time.now
    operation_duration = operation_end_time - operation_start_time
    upload_duration = upload_time_end - upload_time_start

    puts
    puts "start\tbatch\tprocs\tthreads"
    puts "#{upload_time_start}\t#{batch_size}\t#{processes}\t#{threads}"
    puts "Data points inserted: #{data.length}"
    puts "Operation duration: #{operation_duration} seconds"
    puts "Upload duration: #{upload_duration} seconds"
    puts "Throughput: #{(data.length / upload_duration).floor} data points/seconds"
    puts "Initial data points in key: #{initial_count}"

    query_durations = []

    polling_start_time = Time.now

    # Continually query the newly created TempoDB series until all data points have shown up
    loop do
      query_start_time = Time.now
      latest_count = get_series_count(client, series.key)
      query_end_time = Time.now
      query_duration = query_end_time - query_start_time
      query_durations << query_duration
      puts "Total data points in key: #{latest_count}"
      puts "Query duration: #{query_duration.round(3)} seconds -- #{(query_end_time - polling_start_time).round(2)} total seconds"

      break if initial_count + data.length <= latest_count

      say "TempoDB has not caught up (missing #{initial_count + data.length - latest_count}) -- Waiting 1.0 seconds to retry", :yellow
      sleep(1.0)
    end

    query_min = query_durations.min
    query_max = query_durations.max
    query_total = query_durations.inject(:+)
    query_count = query_durations.length
    query_avg = query_total.to_f / query_count # to_f so we don't get an integer result
    query_durations_sorted = query_durations.sort
    query_median = query_count % 2 == 1 ? query_durations_sorted[query_count/2] : (query_durations_sorted[query_count/2 - 1] + query_durations_sorted[query_count/2]).to_f / 2



    puts
    puts "batch\tprocs\tthreads\tstart"
    puts "#{batch_size}\t#{processes}\t#{threads}\t#{upload_time_start}"

    puts
    puts "min\tmax\tavg\tmedian\ttotal\tcount"

    puts "#{query_min.round(3)}\t#{query_max.round(3)}\t#{query_avg.round(3)}\t#{query_median.round(3)}\t#{query_total.round(3)}\t#{query_count}"

  end

  desc 'sample1 KEY', 'Generate the first sample dataset described in SCIV-4'
  def sample1(key)
    start_time = Time.now
    end_time = start_time + (30 * 60)

    spike_up = DataGen::Normal.new(value: 1_000, stddev: 15, delay: 10 * 60)
    spike_down = DataGen::Normal.new(value: -1_000, stddev: 15, delay: 20 * 60)
    baseline = DataGen::Line.new(offset: 10, tolerance: 1)
    series = DataGen::TimeSeries.new(baseline + spike_up + spike_down,
                                     start_time: start_time, end_time: end_time)
    series.frequency = 1

    client = get_tempodb_client
    db_series = check_existing_set(client, key, force: true)

    # db_series = client.create_series(key)
    data = series.map { |t, v| TempoDB::DataPoint.new(t, v) }
    write_in_parallel(data, db_series.key)


    # data.each_slice(1_000).each do |d|
    #   say "Writing #{d.length} data points."
    #   client.write_key(db_series.key, d)
    # end
  end

  desc 'sample2 KEY', 'Generate the second sample dataset described in SCIV-4'
  def sample2(key)
    start_time = Time.now
    end_time = start_time + (30 * 60)

    position = DataGen::Power.new(power: 1.5) +
      DataGen::Sin.new(period: 600, amplitude: 10)
    velocity = DataGen::Power.new(power: 0.5, scale: 1.5) +
      DataGen::Sin.new(period: 600, delay: 150, amplitude: 10)
    acceleration = DataGen::Power.new(power: -0.5, scale: 0.75) +
      DataGen::Sin.new(period: 600, delay: 300, amplitude: 10)
    pos_series = DataGen::TimeSeries.new(position, start_time: start_time,
                                         end_time: end_time)
    vel_series = DataGen::TimeSeries.new(velocity, start_time: start_time,
                                         end_time: end_time)
    acc_series = DataGen::TimeSeries.new(acceleration, start_time: start_time,
                                         end_time: end_time)
    pos_series.frequency = vel_series.frequency = acc_series.frequency = 1

    client = get_tempodb_client

    [[pos_series, {name: "Position", frequency: "1 Hz"}, %w|sample red|],
     [vel_series, {name: "Velocity", frequency: "1 Hz"}, %w|sample blue|],
     [acc_series, {name: "Acceleration", frequency: "1 Hz"}, %w|sample green|]]
      .each do |(data, attrs, tags)|
      sub_series_key = "#{key}.#{attrs[:name]}"
      sub_series = check_existing_set(client, sub_series_key, force: true)
      # sub_series = client.create_series(sub_series_key)
      sub_series.attributes = attrs
      sub_series.tags = tags
      client.update_series(sub_series)
      say "Writing data for key: #{sub_series_key}"
      client.write_key(sub_series_key, data.map { |t, v| TempoDB::DataPoint.new(t, v) })
    end
  end

  desc 'sample3 KEY', 'Generate the third sample dataset described in SCIV-4'
  def sample3(key)
    # Three series that are from different gauges all capturing the same data... but there
    # is a 1% error rate and some data is missing from each gauge. Include tags/attributes
    # for easy identification.
    start_time = Time.now
    end_time = start_time + (30 * 60)
    gen = DataGen::Square.new(period: 500, pct_error: 1) +
      DataGen::Sin.new(period: 300, pct_error: 1) +
      DataGen::Line.new(slope: 0.004, pct_error: 1, offset: 5)

    ser1 = DataGen::TimeSeries.new(gen, start_time: start_time, end_time: end_time,
                                   gap_freq: 0.02, gap_size: 50)
    ser2 = DataGen::TimeSeries.new(gen, start_time: start_time, end_time: end_time,
                                   gap_freq: 0.01, gap_size: 100)
    ser3 = DataGen::TimeSeries.new(gen, start_time: start_time, end_time: end_time,
                                   gap_freq: 0.10, gap_size: 10)
    ser1.frequency = ser2.frequency = ser3.frequency = 3

    client = get_tempodb_client

    [ser1, ser2, ser3].each.with_index do |data, idx|
      sub_series_key = "#{key}.#{idx}"
      check_existing_set(client, sub_series_key, force: true)
      say "Writing data series ##{idx + 1}."
      client.write_key(sub_series_key, data.map { |t, v| TempoDB::DataPoint.new(t, v) })
    end
  end


  private

  # Returns a series object or aborts
  # @param [TempoDB::Client] client
  # @param [String] key
  # @param [Symbol] options
  # @return [TempoDB::Series]
  def check_existing_set(client, key, options)
    does_not_exist = true

    begin
      series = client.get_series(key)
      does_not_exist = false
    rescue TempoDB::TempoDBClientError => msg
      if msg.to_s != "Error: 403 Forbidden" # 403 Forbidden means key doesn't exist
        puts msg.to_s
      end
    end


    if does_not_exist
      series = client.create_series(key)
    else
      if options[:force]
        say "Series for key \"#{key}\" already exists. --force was used so all data for this key has been deleted", :yellow
        client.delete_series(key: key)
        series = client.create_series(key)

      elsif options[:append]
        say "Series for key \"#{key}\" already exists. --append was used so data will be added to existing series", :yellow
        # series is already set

      else
        say "Series for key \"#{key}\" already exists. (Use --force to overwrite or --append to add new data to existing series)", :red
        abort
        return nil
      end
    end

    return series
  end

  # @return [Range]
  def generate_data(start_time, end_time, count, graph_type: 'random')
    duration = end_time - start_time
    interval = (end_time.to_f - start_time.to_f) / count


    # CSV.open("myfile.csv", "w") do |csv|
    #   Range.new(start_time, end_time).step(interval).map do |t|
    #     TempoDB::DataPoint.new(Time.at(t).utc, rand(100))
    #     csv << [Time.at(t).utc.to_datetime.rfc3339(3), rand(100)]
    #   end
    # end


    Range.new(start_time, end_time).step(interval).map do |t|
      case graph_type
        when 'sin'
          value = Math.sin( Math::PI / duration * NUM_OF_SINES * t )
        else
          value = rand(100)
      end
      # value = random(100) # TempoDB::DataPoint.new(Time.at(t).utc, rand(100))

      TempoDB::DataPoint.new(Time.at(t).utc, value)
    end

  end

  # @return [TempoDB::Client]
  def get_tempodb_client
    # TempoDB::Client.new(ENV['TEMPODB_API_KEY'], ENV['TEMPODB_API_SECRET'])
    TempoDB::Client.new('fisi', 'a68ffbe8f6fe4fb3bbda2782002680f0', '3fe37f49b1bb4ae481dec13932c9bb92')
  end

  # @param [TempoDB::Client] client
  # @return [Integer]
  def get_series_count(client, key)
    #Choose arbitrarily early time for first in the series
    query_start = Time.utc(1899, 1, 1)

    #Choose arbitrarily late time for last in the series
    query_end = Time.utc(2020, 1, 1)

    #More details here on reading data from TempoDB: https://tempo-db.com/docs/api/read/
    response = client.get_summary(key, query_start, query_end)

    response.summary['count'] || 0
  end

  # @param data [Range]
  # @param key [String]
  def write_in_parallel(data, key, batch_size: 2000, processes: 1, threads: 4)

    data_array = []
    data.each_slice(batch_size) { |d| data_array << d}

    upload_time_start = Time.now

    Parallel.each(data_array, in_processes: processes, in_threads: threads) do |d|
      say "#{Time.now} -  Writing #{d.length} data points."
      client_p = get_tempodb_client
      client_p.write_data(key, d)
    end

    upload_time_end = Time.now

    return upload_time_start, upload_time_end

  end


end

DataGenCLI.start(ARGV)

# vim:ft=ruby